{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-ae822b978b700c32",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "# Part 4: \"Online\" linear regression\n",
                "\n",
                "When you are trying to fit a model to data and you get to see all of the data at once, we refer to the problem as an _offline_ or _batch_ problem, and you would try to use certain algorithms to compute the fit that can take advantage of the fact that you have a lot of available data.\n",
                "\n",
                "But what if you only get to see one or a few data points at a time? In that case, you might want to get an initial model from whatever data you've got, and gradually improve the model as you see new data points. In this case, we refer to the problem as being an _online_ problem.\n",
                "\n",
                "The goal of this notebook is to introduce you to online algorithms. You'll start by reviewing the offline linear regression problem, and then look at its online variant. The neat thing about the online method is that you can derive it using all the tools you already have at your disposal, namely, multivariate calculus."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-76e9aaecf6da8142",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-a74697ce3692dcbe",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Review: Offline or batch linear regression\n",
                "\n",
                "Let's start with a quick review of the linear regression problem: given a response vector, $y$, and a data matrix $X$---whose rows are observations and columns are variables---the problem is to find the best linear model, $y \\approx X \\theta^*$, where $\\theta^*$ is the vector of best-fit model parameters that we wish to compute. Computing it using a conventional batch linear least squares method has an asymptotic running time of $\\mathcal{O}(mn^2)$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-b753a6dc289159c5",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "To start, here is some code to help generate synthetic problems of a certain size, namely, $m \\times (n+1)$, where $m$ is the number of observations and $n$ the number of predictors. The $+1$ comes from our usual dummy coefficient for a non-zero intercept."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-a013b10e1d1d2d36",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "def generate_model (n):\n",
                "    \"\"\"Returns a set of (random) n+1 linear model coefficients.\"\"\"\n",
                "    return np.random.rand (n+1, 1)\n",
                "\n",
                "def generate_data (m, theta, sigma=1.0\/(2**0.5)):\n",
                "    \"\"\"\n",
                "    Generates 'm' noisy observations for a linear model whose\n",
                "    predictor (non-intercept) coefficients are given in 'theta'.\n",
                "    Decrease 'sigma' to decrease the amount of noise.\n",
                "    \"\"\"\n",
                "    assert (type (theta) is np.ndarray) and (theta.ndim == 2) and (theta.shape[1] == 1)\n",
                "    n = len (theta)\n",
                "    X = np.random.rand (m, n)\n",
                "    X[:, 0] = 1.0\n",
                "    y = X.dot (theta) + sigma*np.random.randn (m, 1)\n",
                "    return (X, y)\n",
                "\n",
                "def estimate_coeffs (X, y):\n",
                "    \"\"\"\n",
                "    Solves X*theta = y by a linear least squares method.\n",
                "    \"\"\"\n",
                "    result = np.linalg.lstsq (X, y, rcond=None)\n",
                "    theta = result[0]\n",
                "    return theta"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-eec5187ec010c42a",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "def rel_diff(x, y, ord=2):\n",
                "    \"\"\"\n",
                "    Computes ||x-y|| \/ ||y||. Uses 2-norm by default;\n",
                "    override by setting 'ord'.\n",
                "    \"\"\"\n",
                "    return np.linalg.norm (x - y, ord=ord) \/ np.linalg.norm (y, ord=ord)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-0f945a8420966877",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## An online algorithm\n",
                "\n",
                "The empirical scaling of linear least squares appears to be pretty good, being roughly linear in $m$ or at worst quadratic in $n$. But there is still a downside in time and storage: each time there is a change in the data, you appear to need to form the data matrix all over again and recompute the solution from scratch, possibly touching the entire data set again!\n",
                "\n",
                "This begs the question, is there a way to incrementally update the model coefficients whenever a new data point, or perhaps a small batch of new data points, arrives? Such a procedure would be considered _incremental_ or _online_, rather than batched or offline."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-495a02d1ac83a302",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Setup: Key assumptions and main goal.** In the discussion that follows, assume that you only get to see the observations _one-at-a-time_. Let $(y_k, \\hat{x}_k^T)$ denote the current observation. (Relative to our previous notation, this tuple is just element $k$ of $y$ and row $k$ of $X$.\n",
                "\n",
                "> We will use $\\hat{x}_k^T$ to denote a row $k$ of $X$ since we previously used $x_j$ to denote column $j$ of $X$. That is,\n",
                ">\n",
                "> $$\n",
                "    X = \\left(\\begin{array}{ccc}\n",
                "          x_0 & \\cdots & x_{n}\n",
                "        \\end{array}\\right)\n",
                "      = \\left(\\begin{array}{c}\n",
                "          \\hat{x}_0^T \\\\\n",
                "            \\vdots \\\\\n",
                "          \\hat{x}_{m-1}^T\n",
                "        \\end{array}\\right),\n",
                "  $$\n",
                ">\n",
                "> where the first form is our previous \"columns-view\" representation and the second form is our \"rows-view.\"\n",
                "\n",
                "Additionally, assume that, at the time the $k$-th observation arrives, you start with a current estimate of the parameters, $\\tilde{\\theta}(k)$, which is a vector. If for whatever reason you need to refer to element $i$ of that vector, use $\\tilde{\\theta}_i(k)$. You will then compute a new estimate, $\\tilde{\\theta}(k+1)$ using $\\tilde{\\theta}(k)$ and $(y_k, \\hat{x}_k^T)$. For the discussion below, further assume that you throw out $\\tilde{\\theta}(k)$ once you have $\\tilde{\\theta}(k+1)$.\n",
                "\n",
                "As for your goal, recall that in the batch setting you start with _all_ the observations, $(y, X)$. From this starting point, you may estimate the linear regression model's parameters, $\\theta$, by solving $X \\theta = y$. In the online setting, you compute estimates one at a time. After seeing all $m$ observations in $X$, your goal is to compute an $\\tilde{\\theta}_{m-1} \\approx \\theta$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-0f00fed6467eed7b",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**An intuitive (but flawed) idea.** Indeed, there is a technique from the signal processing literature that we can apply to the linear regression problem, known as the _least mean square (LMS) algorithm_. Before describing it, let's start with an initial idea.\n",
                "\n",
                "Suppose that you have a current estimate of the parameters, $\\theta(k)$, when you get a new sample, $(y_k, \\hat{x}_k^T)$. The error in your prediction will be,\n",
                "\n",
                "$$y_k - \\hat{x}_k^T \\tilde{\\theta}(k).$$\n",
                "\n",
                "Ideally, this error would be zero. So, let's ask if there exists a _correction_, $\\Delta_k$, such that\n",
                "\n",
                "$$\n",
                "\\begin{array}{rrcl}\n",
                "     & y_k - \\hat{x}_k^T \\left( \\tilde{\\theta}(k) + \\Delta_k \\right) & = & 0 \\\\\n",
                "\\iff &                           y_k - \\hat{x}_k^T \\tilde{\\theta}(k) & = & \\hat{x}_k^T \\Delta_k\n",
                "\\end{array}\n",
                "$$\n",
                "\n",
                "Then, you could compute a new estimate of the parameter by $\\tilde{\\theta}(k+1) = \\tilde{\\theta}(k) + \\Delta_k$.\n",
                "\n",
                "This idea has a major flaw, which we will discuss below. But before we do, please try the following exercise."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-0e0a2b5c75a065f5",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Mental exercise (no points).** Verify that the following choice of $\\Delta_k$ would make the preceding equation true.\n",
                "\n",
                "$$\n",
                "\\begin{array}{rcl}\n",
                "  \\Delta_k & = & \\dfrac{\\hat{x}_k}{\\|\\hat{x}_k\\|_2^2} \\left( y_k - \\hat{x}_k^T \\tilde{\\theta}(k) \\right).\n",
                "\\end{array}\n",
                "$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-f2e525ee3a531640",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Refining (or rather, \"hacking\") the basic idea: The least mean square (LMS) procedure.** The basic idea sketched above has at least one major flaw: the choice of $\\Delta_k$ might allow you to correctly predict $y_k$ from $x_k$ and the new estimate $\\tilde{\\theta}(k+1) = \\tilde{\\theta}(k) + \\Delta_k$, but there is no guarantee that this new estimate $\\tilde{\\theta}(k+1)$ preserves the quality of predictions made at all previous iterations!\n",
                "\n",
                "There are a number of ways to deal with this problem, which includes carrying out an update with respect to some (or all) previous data. However, there is also a simpler \"hack\" that, though it might require some parameter tuning, can be made to work in practice."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-d0e7ca1c5c2e2c67",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "That hack is as follows. Rather than using $\\Delta_k$ as computed above, let's compute a different update that has a \"fudge\" factor, $\\phi$:\n",
                "\n",
                "$$\n",
                "\\begin{array}{rrcl}\n",
                "  &\n",
                "  \\tilde{\\theta}(k+1) & = & \\tilde{\\theta}(k) + \\Delta_k\n",
                "  \\\\\n",
                "  \\mbox{where}\n",
                "  &\n",
                "  \\Delta_k & = & \\phi \\cdot \\hat{x}_k \\left( y_k - \\hat{x}_k^T \\tilde{\\theta}(k) \\right).\n",
                "\\end{array}\n",
                "$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-f36e2f688273c7c2",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "A big question is how to choose $\\phi$. There is some analysis out there that can help. We will just state the results of this analysis without proof.\n",
                "\n",
                "Let $\\lambda_{\\mathrm{max}}(X^T X)$ be the largest eigenvalue of $X^T X$. The result is that as the number of samples $s \\rightarrow \\infty$, any choice of $\\phi$ that satisfies the following condition will _eventually_ converge to the best least-squares estimator of $\\tilde{\\theta}$, that is, the estimate of $\\tilde{\\theta}$ you would have gotten by solving the linear least squares problem with all of the data.\n",
                "\n",
                "$$\n",
                "  0 < \\phi < \\frac{2}{\\lambda_{\\mathrm{max}}(X^T X)}.\n",
                "$$\n",
                "\n",
                "This condition is not very satisfying, because you cannot really know $\\lambda_{\\mathrm{max}}(X^T X)$ until you've seen all the data, whereas we would like to apply this procedure _online_ as the data arrive. Nevertheless, in practice you can imagine hybrid schemes that, given a batch of data points, use the QR fitting procedure to get a starting estimate for $\\tilde{\\theta}$ as well as to estimate a value of $\\phi$ to use for all future updates."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-fd2a4be161147caa",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Summary of the LMS algorithm.** To summarize, the algorithm is as follows:\n",
                "* Choose any initial guess, $\\tilde{\\theta}(0)$, such as $\\tilde{\\theta}(0) \\leftarrow 0$.\n",
                "* For each observation $(y_k, \\hat{x}_k^T)$, do the update:\n",
                "\n",
                "  * $\\tilde{\\theta}(k+1) \\leftarrow \\tilde{\\theta}_k + \\Delta_k$,\n",
                "  \n",
                "  where $\\Delta_k = \\phi \\cdot \\hat{x}_k \\left( y_k - \\hat{x}_k^T \\tilde{\\theta}(k) \\right)$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-1c8224a488dc995c",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Trying out the LMS idea\n",
                "\n",
                "Now _you_ should implement the LMS algorithm and see how it behaves.\n",
                "\n",
                "To start, let's generate an initial 1-D problem (2 regression coefficients, a slope, and an intercept), and solve it using the batch procedure."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-d7d3b934f53af706",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Recall that we need a value for $\\phi$, for which we have an upper-bound of $\\lambda_{\\mathrm{max}}(X^T X)$. Let's cheat by computing it explicitly, even though in practice we would need to do something different."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-3e4219c8414d5443",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "m = 100000\n",
                "n = 1\n",
                "theta_true = generate_model(n)\n",
                "\n",
                "(X, y) = generate_data(m, theta_true, sigma=0.1)\n",
                "\n",
                "print(\"Condition number of the data matrix:\", np.linalg.cond(X))\n",
                "\n",
                "theta = estimate_coeffs(X, y)\n",
                "e_rel = rel_diff(theta, theta_true)\n",
                "\n",
                "print(\"Relative error:\", e_rel)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-4fc601f5b57a006f",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "LAMBDA_MAX = max(np.linalg.eigvals(X.T.dot(X)))\n",
                "print(LAMBDA_MAX)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-2e86f046faca4191",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Exercise 1** (5 points). Implement the online LMS algorithm in the code cell below where indicated. It should produce a final parameter estimate, `theta_lms`, as a column vector.\n",
                "\n",
                "In addition, the skeleton code below uses `rel_diff()` to record the relative difference between the estimate and the true vector, storing the $k$-th relative difference in `rel_diffs[k]`. Doing so will allow you to see the convergence behavior of the method.\n",
                "\n",
                "Lastly, to help you out, we've defined a constant in terms of $\\lambda_{\\mathrm{max}}(X^T X)$ that you can use for $\\phi$.\n",
                "\n",
                "> In practice, you would only maintain the current estimate, or maybe just a few recent estimates, rather than all of them. Since we want to inspect these vectors later, go ahead and store them all."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "lms",
                    "locked": false,
                    "schema_version": 1,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "PHI = 1.99 \/ LAMBDA_MAX # Fudge factor\n",
                "rel_diffs = np.zeros((m+1, 1))\n",
                "\n",
                "theta_k = np.zeros((n+1))\n",
                "for k in range(m):\n",
                "    rel_diffs[k] = rel_diff(theta_k, theta_true)\n",
                "\n",
                "    # Implement the online LMS algorithm.\n",
                "    # Take (y[k], X[k, :]) to be the k-th observation.\n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n",
                "    \n",
                "theta_lms = theta_k\n",
                "rel_diffs[m] = rel_diff(theta_lms, theta_true)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-486af1b126a34491",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Let's compare the true coefficients against the estimates, both from the batch algorithm and the online algorithm. The values of the variables below might change if the notebooks are re-run from start."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "lms_test",
                    "locked": true,
                    "points": 5,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "print (theta_true.T)\n",
                "print (theta.T)\n",
                "print (theta_lms.T)\n",
                "\n",
                "print(\"\\n('Passed' -- this cell appears to run without error, but we aren't checking the solution.)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-cbeeadccc0a71818",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Let's also compute the relative differences between each estimate `Theta[:, k]` and the true coefficients `theta_true`, measured in the two-norm, to see if the estimate is converging to the truth."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-1e3a7e61238da6b6",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "plt.plot(range(len(rel_diffs)), rel_diffs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-a992e44f89ef77eb",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "You should see it converging, but not especially quickly.\n",
                "\n",
                "Finally, if the dimension is `n=1`, let's go ahead and do a sanity-check regression fit plot. The plot can change if the notebooks are re-run from start."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "STEP = int(X.shape[0] \/ 500)\n",
                "if n == 1:\n",
                "    fig = plt.figure()\n",
                "    ax1 = fig.add_subplot(111)\n",
                "    ax1.plot(X[::STEP, 1], y[::STEP], 'b+') # blue - data\n",
                "    ax1.plot(X[::STEP, 1], X.dot(theta_true)[::STEP], 'r*') # red - true\n",
                "    ax1.plot(X[::STEP, 1], X.dot(theta)[::STEP], 'go') # green - batch\n",
                "    ax1.plot(X[::STEP, 1], X.dot(theta_lms)[::STEP], 'mo') # magenta - pure LMS\n",
                "else:\n",
                "    print(\"Plot is multidimensional; I live in Flatland, so I don't do that.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "**Exercise 2** (_ungraded_, optional). We said previously that, in practice, you would probably do some sort of _hybrid_ scheme that mixes full batch updates (possibly only initially) and incremental updates. Implement such a scheme and describe what you observe. You might observe a different plot each time the cell is re-run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "hybrid",
                    "locked": false,
                    "schema_version": 1,
                    "solution": true
                },
                "scrolled": false
            },
            "outputs": [],
            "source": [
                "# Setup problem and compute the batch solution\n",
                "m = 100000\n",
                "n = 1\n",
                "theta_true = generate_model(n)\n",
                "(X, y) = generate_data(m, theta_true, sigma=0.1)\n",
                "theta_batch = estimate_coeffs(X, y)\n",
                "\n",
                "# Your turn, below: Implement a hybrid batch-LMS solution\n",
                "# assuming you observe the first few data points all at\n",
                "# once, and then see the remaining points one at a time.\n",
                "\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Fin!** If you've gotten this far without errors, your notebook is ready to submit."
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Create Assignment",
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python38"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}